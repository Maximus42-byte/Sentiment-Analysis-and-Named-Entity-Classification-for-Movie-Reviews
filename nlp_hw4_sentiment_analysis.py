# -*- coding: utf-8 -*-
"""NLP_HW4_sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EY7-W-TG8Hz9gE5BwRc6nefP8F28CWpa

# Data observe & Preparation

<div dir = "rtl" style = 'font-family: "B Zar";' align = "center">
    <h1 align = "center">
        به نام خدا
    </h1>
    <h3 align = "center">
        تمرین چهارم پردازش زبان - تحلیل احساس مدل برت
    </h3>
    <h3 align = "center">
        استاد: دکتر عسگری
    </h3>
    <h3 align = "center">
         محمدرضا کمالی -      
         مهدی سعیدی  
    </h3>

</div>

<div dir = "rtl" style = 'font-family: "B Zar";'>

  <p>
کتابخانه ها و ابزار های مورد نیاز را نصب و اضافه می کنیم
 </p>
</div>
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pytreebank

"""<div dir = "rtl" style = 'font-family: "B Zar";'>

  <p>با استفاده از کتابخانه pytreebank دیتا ست گفته شده را لود میکنیم و سلول های زیر یه نمایش بصری از توابع و نحوه ذخیره سازی درختی این داده را نمایش میدهد و هر کدام از توابع را به طور مجزا خروجی گرفته ایم </p>
</div>

"""

import pytreebank
# load the sentiment treebank corpus in the parenthesis format,
# e.g. "(4 (2 very ) (3 good))"
dataset = pytreebank.load_sst()
# add Javascript and CSS to the Ipython notebook
pytreebank.LabeledTree.inject_visualization_javascript()
# select and example to visualize
example = dataset["test"][2000]
# display it in the page
example.display()

import pytreebank
dataset = pytreebank.load_sst()
# example = dataset["test"][1]

# extract spans from the tree.
for label, sentence in example.to_labeled_lines():
	print("%s has sentiment label %s" % (
		sentence,
		["very negative", "negative", "neutral", "positive", "very positive"][label]
	))

example.to_lines()[0]

len(example.to_lines()[0].split())

example.label

sst_train = dataset["train"]
sst_val = dataset["dev"]
sst_test = dataset["test"]

for i in range(len(dataset["train"])):
  print(dataset["train"][i].to_lines()[0])
  print(dataset["train"][i].label)

# %pip install -q transformers
# %pip install -q hazm
# %pip install -q clean-text[gpl]
# %pip install -q plotly

# import hazm
# from cleantext import clean
# from string import punctuation

# import plotly.graph_objects as go

# # calculate the length of news based on their words
# dataset['text_len_by_words'] = dataset["train"].apply(lambda t: len(hazm.word_tokenize(t)))

"""# **Bert** Model

## Libraries and Tools
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pytreebank

# Commented out IPython magic to ensure Python compatibility.
# %pip install pytorch_transformers

import pytreebank
import torch
from pytorch_transformers import BertTokenizer,BertConfig, BertForSequenceClassification
from torch.utils.data import Dataset
from tqdm import tqdm

"""## Load data

<div dir = "rtl" style = 'font-family: "B Zar";'>
  
  <p>حال ما نیاز داریم تا کلاسی برای دادگان بسازیم و همچنین لازم هست به یک فرم یکسان دادگان یونیفرم شوند تا طول برابر برای ورود به ترنسفرمر داشته باشند که ما این طول را 66 گرفته ایم. همچنین ما  در این کد دیتاست تحلیل احساس را به صورت طبقه بندی ریز دانه 5 کلاسه و با داشتن کل جمله نظرات فیلم و صرفا ریشه درخت دسته بندی کردیم. </p>
</div>
"""

tokenizer = BertTokenizer.from_pretrained("bert-large-uncased")

sst = pytreebank.load_sst()

def uniformer(array):
    if len(array) > 66:
        return array[: 65]
    return array + ([0] * (66 - len(array)))

class datacreate():

    def __init__(self, split="train"):
        self.sst = sst[split]
        self.data = [(uniformer(tokenizer.encode("[CLS] " + tree.to_lines()[0] + " [SEP]")),tree.label) for tree in self.sst]
    def __getitem__(self, index):
        sentence, lable = self.data[index]
        sentence = torch.tensor(sentence)
        return sentence, lable
    def __len__(self):
        return len(self.data)

trainset = datacreate("train")

trainset[2000]

"""## Training phase

<div dir = "rtl" style = 'font-family: "B Zar";'>
  <p>حال مرحله آموزش مدل ما است. مدل ما بر روی دادگان اموزش ، آموزش می بیند و در هر اپوک با دادگان تست مرود ارزیابی قرار میگرد و 4 معیار گفته شده را در هر مرحله گزارش میکنیم . ما از مدل "bert-large-uncased" برای فاین تیون کردن استفاده کردیم و همچنین قابلیت سیو مدل در گوگل درایو نیز ایجاد شده است که با دادن پارامتر به تابع میتوان مدل را سیو کرد در مسیر گوگل درایو.  </p>
  <p>دقت مدل ما روی دادگان تست به حدود 50 درصد میرسو که با بیس لاین این تسک که حدود 60 درصد است کمی فاصله دارد. </p>
</div>
"""

def train(bert="bert-large-uncased",epochs=5,batch_size=32,save=False):

    trainset = datacreate("train")
    testset = datacreate("test")

    device = torch.device("cuda")
    # device = torch.device("cpu")

    config = BertConfig.from_pretrained(bert)
    config.num_labels = 5
    model = BertForSequenceClassification.from_pretrained(bert, config=config)
    model = model.to(device)

    for epoch in range(1, epochs+1):
        print("_"*100)
        print(f"epoch={epoch}")
        ## train ##

        lossfn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
        generator = torch.utils.data.DataLoader(trainset, batch_size=batch_size)
        model.train()
        train_loss, train_acc = 0.0, 0.0
        precision_numerator = [0.0 , 0.0 ,0.0 ,0.0 ,0.0]
        precision_denominator = [0.0 , 0.0 ,0.0 ,0.0 ,0.0]
        recall_numerator = [0.0 , 0.0 ,0.0 ,0.0 ,0.0]
        recall_denominator = [0.0 , 0.0 ,0.0 ,0.0 ,0.0]
        for batch, labels in tqdm(generator):
            batch, labels = batch.to(device), labels.to(device)
            optimizer.zero_grad()
            loss, pred = model(batch, labels=labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            pred_labels = torch.argmax(pred, axis=1)
            for i in range(len(pred_labels)):
                precision_numerator[pred_labels[i]] += (pred_labels[i] == labels[i])
                precision_denominator[pred_labels[i]] += 1
                recall_numerator[pred_labels[i]] += (pred_labels[i] == labels[i])
                recall_denominator[labels[i]] += 1
            train_acc += (pred_labels == labels).sum().item()
        recall = 0.0
        precision = 0.0
        f1 = 0.0
        count_precision = len(recall_numerator)
        count_recall = len(recall_numerator)
        for i in range(len(recall_numerator)):
          if (precision_denominator[i] > 0.0):
            p = precision_numerator[i]/precision_denominator[i]
            precision += p
          else:
            p = 0.0
            count_precision -= 1
          if (recall_denominator[i] > 0.0):
            r = recall_numerator[i]/recall_denominator[i]
            recall += r
          else:
            r = 0.0
            count_recall -= 1
          if r>0.0 or p>0.0:
            f1 += (2*r*p)/(r+p)
          else:
            f1 += 0.0
        if count_precision > 0 :
          precision /= count_precision
        if count_recall > 0 :
          recall /= count_recall      
        print(f"train loss={train_loss/len(trainset)} , train acc={(train_acc/len(trainset))*100:.2f}%")
        print("train precision_score: {}".format(precision))
        print("train recall_score: {}".format(recall))
        print("train F1-Score: {}".format(f1/len(recall_numerator)))
        
        ## test ##
        lossfn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
        generator = torch.utils.data.DataLoader(testset, batch_size=batch_size)
        model.eval()
        test_loss, test_acc = 0.0, 0.0
        precision_numerator = [0.0 , 0.0 ,0.0 ,0.0 ,0.0]
        precision_denominator = [0.0 , 0.0 ,0.0 ,0.0 ,0.0]
        recall_numerator = [0.0 , 0.0 ,0.0 ,0.0 ,0.0]
        recall_denominator = [0.0 , 0.0 ,0.0 ,0.0 ,0.0]
        with torch.no_grad():
            for batch, labels in tqdm(generator):
                batch, labels = batch.to(device), labels.to(device)
                logits = model(batch)[0]
                fault = lossfn(logits, labels)
                test_loss += fault.item()
                pred_labels = torch.argmax(logits, axis=1)
                test_acc += (pred_labels == labels).sum().item()
                for i in range(len(pred_labels)):
                  precision_numerator[pred_labels[i]] += (pred_labels[i] == labels[i])
                  precision_denominator[pred_labels[i]] += 1
                  recall_numerator[pred_labels[i]] += (pred_labels[i] == labels[i])
                  recall_denominator[labels[i]] += 1
        recall = 0.0
        precision = 0.0
        f1 = 0.0
        count_precision = len(recall_numerator)
        count_recall = len(recall_numerator)
        for i in range(len(recall_numerator)):
          if (precision_denominator[i] > 0.0):
            p = precision_numerator[i]/precision_denominator[i]
            precision += p
          else:
            p = 0.0
            count_precision -= 1
          if (recall_denominator[i] > 0.0):
            r = recall_numerator[i]/recall_denominator[i]
            recall += r
          else:
            r = 0.0
            count_recall -= 1
          if r>0.0 or p>0.0:
            f1 += (2*r*p)/(r+p)
          else:
            f1 += 0.0
        if count_precision > 0 :
          precision /= count_precision
        if count_recall > 0 :
          recall /= count_recall      
        print(f"test loss={test_loss/len(testset)} , test acc={(test_acc/len(testset))*100:.2f}%")
        print("test precision_score: {}".format(precision))
        print("test recall_score: {}".format(recall))
        print("test F1-Score: {}".format(f1/len(recall_numerator)))
        ## saving ##

        if save:
            model_save_name = f"{bert}__e{epoch}.pickle"
            path = f"/content/drive/MyDrive/{model_save_name}" 
            torch.save(model, path)

    return model

model = train(bert="bert-large-uncased",epochs=4,batch_size=35)

"""## Test Model

<div dir = "rtl" style = 'font-family: "B Zar";'>

  <p>سپس با گرفتن کوئری میتوان خروجی مدل آموزش دیدمان را بر روی این جمله مشاهده کنیم که به کدام کلاس ریز دانه تخصیص دارد. </p>
</div>
"""

query = "Professor gave justice to his role. What a legend. you'll start admiring some characters. Who thought a TV series on a money heist will be so interesting."

# query = "this film was so nice , wonderful , great and awesome."

sentiment = ["very negative", "negative", "neutral", "positive", "very positive"]
tokenized_query = uniformer(tokenizer.encode("[CLS] " + query + " [SEP]"))
model_query = torch.tensor([tokenized_query]).cuda()
output = model(model_query)[0]
label_pred = torch.argmax(output, axis=1)
print(output)
print(label_pred)
print(sentiment[label_pred])